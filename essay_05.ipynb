{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essay 5: \n",
    "\n",
    "# A Better Benchmark for Hierarchical Risk Parity (HRP) Portfolios: Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "    I. Motivation & Summary\n",
    "    II. HRP Walkthrough\n",
    "    III. Revisiting the JoPM 10-Asset Example\n",
    "    Appendix A: Version Control / GitHub\n",
    "    Appendix B: Quick Comment on How to do Research\n",
    "    Appendix C: Revisiting the Appendixes of Essay 1\n",
    "    Appendix D: Solving Mystery 1\n",
    "    Appendix E: Solving Mystery 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I.\tMotivation & Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://i.imgur.com/YLWlLbW.png](https://i.imgur.com/YLWlLbW.png \"is_this_ML_title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. HRP Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a continuation of <a href=\"https://kyleessays.substack.com/p/essay-1-a-better-benchmark-for-hierarchical\">Essay 1</a>, which is a critique of <a href=\"https://jpm.pm-research.com/content/42/4/59\">Building Diversified Portfolios that Outperform Out-of-Sample</a> (BDPOOS), published by <a href=\"https://jpm.pm-research.com/quant-of-2019\">2019 Quant of the Year Marcos LÃ³pez de Prado</a> (MLdP) in the Summer 2016 Issue of The Journal of Portfolio Management (JoPM).\n",
    "\n",
    "Warning: Like Essay 1, this is mostly a math essay.\n",
    "\n",
    "Here's a quick summary of how HRP weights are computed:\n",
    "1.  Convert covariance matrix to a correlation matrix.\n",
    "2.  Convert the correlation matrix to a distance matrix.\n",
    "3.  Cluster the distances into groups.\n",
    "4.  Sort the original covariance matrix based on those groupings.\n",
    "5a.  Recursively bisect that sorted covariance matrix.\n",
    "5b.  For each bisection, apply IVP weights to that subset.\n",
    "\n",
    "In the case of the 10-Asset example from JoPM, the 10 assets are artificially constructed to be correlated to each other: \n",
    "\n",
    "![https://i.imgur.com/gIGOG7K.png](https://i.imgur.com/gIGOG7K.png \"table_2\")\n",
    "\n",
    "The above assets clearly fall neatly into the following clusters: {9, 2, 10}, {1, 7}, {3, 6}, {4}, and {5, 8}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first bisection would carve the above into these two super-sets: {9, 2, 10, 1, 7} and {3, 6, 4, 5, 8}.\n",
    "\n",
    "But what if we didn't use asset 4, which is uncorrelated to anything else...\n",
    "\n",
    "Then the first bisection might give us the super-sets {9, 2, 10, 1} and {7, 3, 6, 5, 8}.\n",
    "\n",
    "Wait what?  Assets 1 & 7 could be separated from each other right at the start?  They have a correlation of 0.97!  You create clusters and then just ignore them at the first step?\n",
    "\n",
    "Let's run some code to confirm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Revisiting the JoPM 10-Asset Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run some code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch, numpy as np, pandas as pd\n",
    "import os\n",
    "import HRP_v5 as hrpm # modified version to include print statements & be Python3 compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kyle\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Load data:\n",
    "cwd = os.getcwd()\n",
    "csv_dir_01 = cwd + '\\\\Simulated_Gaussians_Seed12345.csv'\n",
    "\n",
    "x = pd.read_csv(csv_dir_01, index_col=0)\n",
    "\n",
    "# Get covariances to be input to HRP:\n",
    "covariances, corr = x.cov(), x.corr()\n",
    "\n",
    "# Convert correlations to distances: \n",
    "dist = hrpm.correlDist(corr)\n",
    "link = sch.linkage(dist, 'single')\n",
    "\n",
    "sortIx = hrpm.getQuasiDiag(link)\n",
    "sortIx = corr.index[sortIx].tolist()  # recover labels\n",
    "df0 = corr.loc[sortIx, sortIx]  # reorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've modified MLdP's HRP.py code (to the HRP_v5.py that you imported above, assuming you properly imported the branch essay_05) as follows:\n",
    "\n",
    "(1) Inserted numerous \"print\" statements so that we can follow the algorithm step-by-step.\n",
    "\n",
    "(2) Remove Python 2 code (such as \"xrange\") so that this can run in this Jupyter Python 3 environment.\n",
    "\n",
    "Let's call HRP on and observe the first bisection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['9', '2', '10', '1', '7', '3', '6', '4', '5', '8']]\n",
      "-----\n",
      "cItems0\n",
      "['9', '2', '10', '1', '7']\n",
      "-----\n",
      "cItems1\n",
      "['3', '6', '4', '5', '8']\n",
      "-----\n",
      "cVar0\n",
      "0.5253929089614129\n",
      "-----\n",
      "cVar1\n",
      "0.3669878839679833\n",
      "-----\n",
      "alpha\n",
      "0.4112458345985701\n",
      "-----\n",
      "cItems0\n",
      "['9', '2']\n",
      "-----\n",
      "cItems1\n",
      "['10', '1', '7']\n",
      "-----\n",
      "cVar0\n",
      "1.0253128128876061\n",
      "-----\n",
      "cVar1\n",
      "0.5713503792630303\n",
      "-----\n",
      "alpha\n",
      "0.35784026466686814\n",
      "-----\n",
      "cItems0\n",
      "['3', '6']\n",
      "-----\n",
      "cItems1\n",
      "['4', '5', '8']\n",
      "-----\n",
      "cVar0\n",
      "1.0160858695552584\n",
      "-----\n",
      "cVar1\n",
      "0.564640754896351\n",
      "-----\n",
      "alpha\n",
      "0.3572032925631514\n",
      "-----\n",
      "cItems0\n",
      "['9']\n",
      "-----\n",
      "cItems1\n",
      "['2']\n",
      "-----\n",
      "cVar0\n",
      "1.0753466770005407\n",
      "-----\n",
      "cVar1\n",
      "1.0090213488712376\n",
      "-----\n",
      "alpha\n",
      "0.48408982307681414\n",
      "-----\n",
      "cItems0\n",
      "['10']\n",
      "-----\n",
      "cItems1\n",
      "['1', '7']\n",
      "-----\n",
      "cVar0\n",
      "1.0785866856713524\n",
      "-----\n",
      "cVar1\n",
      "1.013014325747844\n",
      "-----\n",
      "alpha\n",
      "0.48432484026219313\n",
      "-----\n",
      "cItems0\n",
      "['3']\n",
      "-----\n",
      "cItems1\n",
      "['6']\n",
      "-----\n",
      "cVar0\n",
      "1.0007393631661812\n",
      "-----\n",
      "cVar1\n",
      "1.0643098248127214\n",
      "-----\n",
      "alpha\n",
      "0.5153919969598297\n",
      "-----\n",
      "cItems0\n",
      "['4']\n",
      "-----\n",
      "cItems1\n",
      "['5', '8']\n",
      "-----\n",
      "cVar0\n",
      "1.0118547250981296\n",
      "-----\n",
      "cVar1\n",
      "1.0233244251460323\n",
      "-----\n",
      "alpha\n",
      "0.5028178600509263\n",
      "-----\n",
      "cItems0\n",
      "['1']\n",
      "-----\n",
      "cItems1\n",
      "['7']\n",
      "-----\n",
      "cVar0\n",
      "1.0006477994290504\n",
      "-----\n",
      "cVar1\n",
      "1.0581720266953933\n",
      "-----\n",
      "alpha\n",
      "0.5139701946077107\n",
      "-----\n",
      "cItems0\n",
      "['5']\n",
      "-----\n",
      "cItems1\n",
      "['8']\n",
      "-----\n",
      "cVar0\n",
      "1.0058358778581784\n",
      "-----\n",
      "cVar1\n",
      "1.0748331551216201\n",
      "-----\n",
      "alpha\n",
      "0.516580550815578\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# Capital allocation\n",
    "hrp_weights = hrpm.getRecBipart(covariances, sortIx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe from the above output that indeed the first cut produces these two super sets:\n",
    "\n",
    "cItems0\n",
    "['9', '2', '10', '1', '7']\n",
    "\n",
    "cItems1\n",
    "['3', '6', '4', '5', '8']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's observe the same bisection if we removed the uncorrelated asset number 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kyle\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['9', '2', '10', '5', '8', '3', '6', '1', '7']]\n",
      "-----\n",
      "cItems0\n",
      "['9', '2', '10', '5']\n",
      "-----\n",
      "cItems1\n",
      "['8', '3', '6', '1', '7']\n",
      "-----\n",
      "cVar0\n",
      "0.6281019660185938\n",
      "-----\n",
      "cVar1\n",
      "0.37268433108485094\n",
      "-----\n",
      "alpha\n",
      "0.37239152071076864\n",
      "-----\n",
      "cItems0\n",
      "['9', '2']\n",
      "-----\n",
      "cItems1\n",
      "['10', '5']\n",
      "-----\n",
      "cVar0\n",
      "1.0253128128876061\n",
      "-----\n",
      "cVar1\n",
      "0.5165043040829743\n",
      "-----\n",
      "alpha\n",
      "0.33499712670061743\n",
      "-----\n",
      "cItems0\n",
      "['8', '3']\n",
      "-----\n",
      "cItems1\n",
      "['6', '1', '7']\n",
      "-----\n",
      "cVar0\n",
      "0.519191812730093\n",
      "-----\n",
      "cVar1\n",
      "0.5763578525427326\n",
      "-----\n",
      "alpha\n",
      "0.5260901178763099\n",
      "-----\n",
      "cItems0\n",
      "['9']\n",
      "-----\n",
      "cItems1\n",
      "['2']\n",
      "-----\n",
      "cVar0\n",
      "1.0753466770005407\n",
      "-----\n",
      "cVar1\n",
      "1.0090213488712376\n",
      "-----\n",
      "alpha\n",
      "0.48408982307681414\n",
      "-----\n",
      "cItems0\n",
      "['10']\n",
      "-----\n",
      "cItems1\n",
      "['5']\n",
      "-----\n",
      "cVar0\n",
      "1.0785866856713524\n",
      "-----\n",
      "cVar1\n",
      "1.0058358778581784\n",
      "-----\n",
      "alpha\n",
      "0.48254893007635025\n",
      "-----\n",
      "cItems0\n",
      "['8']\n",
      "-----\n",
      "cItems1\n",
      "['3']\n",
      "-----\n",
      "cVar0\n",
      "1.0748331551216201\n",
      "-----\n",
      "cVar1\n",
      "1.000739363166181\n",
      "-----\n",
      "alpha\n",
      "0.48215099898880887\n",
      "-----\n",
      "cItems0\n",
      "['6']\n",
      "-----\n",
      "cItems1\n",
      "['1', '7']\n",
      "-----\n",
      "cVar0\n",
      "1.0643098248127214\n",
      "-----\n",
      "cVar1\n",
      "1.013014325747844\n",
      "-----\n",
      "alpha\n",
      "0.487653467791477\n",
      "-----\n",
      "cItems0\n",
      "['1']\n",
      "-----\n",
      "cItems1\n",
      "['7']\n",
      "-----\n",
      "cVar0\n",
      "1.0006477994290504\n",
      "-----\n",
      "cVar1\n",
      "1.0581720266953933\n",
      "-----\n",
      "alpha\n",
      "0.5139701946077107\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "y = pd.read_csv(csv_dir_01, index_col=0)\n",
    "y = y.iloc[:,[0,1,2,4,5,6,7,8,9]]\n",
    "\n",
    "# Get covariances to be input to HRP:\n",
    "covariances, corr = y.cov(), y.corr()\n",
    "\n",
    "# Convert correlations to distances: \n",
    "dist = hrpm.correlDist(corr)\n",
    "link = sch.linkage(dist, 'single')\n",
    "\n",
    "sortIx = hrpm.getQuasiDiag(link)\n",
    "sortIx = corr.index[sortIx].tolist()  # recover labels\n",
    "df0 = corr.loc[sortIx, sortIx]  # reorder\n",
    "\n",
    "# Capital allocation\n",
    "hrp_weights = hrpm.getRecBipart(covariances, sortIx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first cut produced these two super sets:\n",
    "\n",
    "cItems0 ['9', '2', '10', '5']\n",
    "\n",
    "cItems1 ['8', '3', '6', '1', '7']\n",
    "\n",
    "\n",
    "Indeed, the cluster {5, 8} is destroyed before we even get to take advantage of its status as a cluster.  \n",
    "\n",
    "That {5, 8} appears before {1, 7} in the example without {4} is a property of the scipy linkage procedure; you can add additional print statements to confirm how this happens.\n",
    "\n",
    "Can this process be made better?  Of course.  That's another essay.  \n",
    "\n",
    "And we didn't even address the silly choice of HRP to apply IVP weights at each recursion, instead of, say, ERC weights.  That is, actually account for correlations that JoPM was so hasty to insist were excluded by their straw-man IVP benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A: Version Control / GitHub\n",
    "\n",
    "Any code, data, and other artifacts produced for this essay will be saved at https://github.com/kyle-binder-essays/essay_repo1/tree/essay_05. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B: Quick Comment on How to do Research\n",
    "\n",
    "Jupyter is well suited for presentation of results.Â  But if youâre not sure what your research results will be and need to run lots of scenarios quickly, a more optimal workflow is to simply open multiple Spyder consoles and test as many variations of code as you desire. Â \n",
    "\n",
    "However, once you need to share and communicate results with a team, the above approach fails to scale.\n",
    "\n",
    "Is there a more optimal balance between doing research and sharing research results more quickly?Â  At first glance, NBDEV seems promising (and Iâm optimistic given previous successes of the fastai team)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix C: Revisiting the Appendixes of Essay 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Appendixes of Essay 1 explored simple examples to better understand the Hierarchical Risk Parity (HRP) and Equal Risk Contribution (ERC) algorithms.  The simple examples presented us with two unintuitive results:\n",
    "\n",
    "**Mystery #1**: \n",
    "\n",
    "HRP can assign very different weights for the same asset when it appears twice in the input variables.  \n",
    "\n",
    "**Mystery #2**: \n",
    "\n",
    "HRP output weights can be very sensitive to small perturbations of inputs.\n",
    "\n",
    "These are unintuitive results, and most finance practitioners would tell you theyâre UNDESIRABLE results.\n",
    "\n",
    "**Mystery #1 Details**: \n",
    "\n",
    "\n",
    "In Figure 5 from <a href=\"https://kyleessays.substack.com/p/essay-1-a-better-benchmark-for-hierarchical\">Essay 1</a>, why does B get much larger weight than A and D?  A, B, and D are the same asset (each has variance = 1.0, and all three of {A,B,D} are 100% correlated to each other).  ERC assigns {A,B,D} the expected behavior of equal weights.\n",
    "\n",
    "Here are Figures 5 and 6 from Essay #1; { X1, X2, â¦ X5} are from the original BDPOOS data set, normalized to have variance = 1.0:\n",
    "\n",
    "\n",
    "![https://i.imgur.com/zK3Ya3h.png](https://i.imgur.com/zK3Ya3h.png \"fig5\")\n",
    "\n",
    "![https://i.imgur.com/yzEzlc0.png](https://i.imgur.com/yzEzlc0.png \"fig6\")\n",
    "\n",
    "\n",
    "**Mystery #2 Details**: \n",
    "\n",
    "\n",
    "In Figure 6 from Essay #1, when we decrease epsilon from 1 to an arbitrarily small positive number (but not zero), the HRP weights converge on a desirable property: HRP allocates equally between the cluster {A,B,D} and the cluster {C,E}.\n",
    "\n",
    "However, when we drop epsilon from 0.01 to 0, there is a large shift in HRP weights (but not ERC weights).  \n",
    "\n",
    "More problematic: HRP no longer allocates equally between the cluster {A,B,D} and the cluster {C,E}; that is, the sum of the weights of A+B+D is not the same as the sum of the weights of C+E.\n",
    "\n",
    "Again, ERC assigns {A,B,D} the expected behavior of (near) equal weights for both epsilon=0 and epsilon=0.1.\n",
    "\n",
    "So, how do these things happen?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix D: Solving Mystery 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run some code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch, numpy as np, pandas as pd\n",
    "import os\n",
    "import HRP_v5 as hrpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "cwd = os.getcwd()\n",
    "csv_dir_01 = cwd + '\\\\Simulated_Gaussians_Seed12345.csv'\n",
    "\n",
    "x_all = pd.read_csv(csv_dir_01, index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's load the data set corresponding to epsilon = 0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['x_C', 'x_E', 'x_B', 'x_A', 'x_D']]\n",
      "-----\n",
      "cItems0\n",
      "['x_C', 'x_E']\n",
      "-----\n",
      "cItems1\n",
      "['x_B', 'x_A', 'x_D']\n",
      "-----\n",
      "cVar0\n",
      "0.9969553731227971\n",
      "-----\n",
      "cVar1\n",
      "0.9945518529188007\n",
      "-----\n",
      "alpha\n",
      "0.49939655749862033\n",
      "-----\n",
      "cItems0\n",
      "['x_C']\n",
      "-----\n",
      "cItems1\n",
      "['x_E']\n",
      "-----\n",
      "cVar0\n",
      "0.9999999999999964\n",
      "-----\n",
      "cVar1\n",
      "0.9999999999999929\n",
      "-----\n",
      "alpha\n",
      "0.4999999999999991\n",
      "-----\n",
      "cItems0\n",
      "['x_B']\n",
      "-----\n",
      "cItems1\n",
      "['x_A', 'x_D']\n",
      "-----\n",
      "cVar0\n",
      "1.0\n",
      "-----\n",
      "cVar1\n",
      "0.9969491877753843\n",
      "-----\n",
      "alpha\n",
      "0.49923613173452497\n",
      "-----\n",
      "cItems0\n",
      "['x_A']\n",
      "-----\n",
      "cItems1\n",
      "['x_D']\n",
      "-----\n",
      "cVar0\n",
      "1.0000000000000013\n",
      "-----\n",
      "cVar1\n",
      "1.0\n",
      "-----\n",
      "alpha\n",
      "0.49999999999999967\n",
      "-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kyle\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n"
     ]
    }
   ],
   "source": [
    "# Iterate over several values of epsilon and collect all outputs into single dataframe:\n",
    "iterator_array = pd.DataFrame(range(0,20,5))\n",
    "# Initialize:\n",
    "figure6 = pd.DataFrame(columns=list(range(10,11,10)), data=None, \\\n",
    "                               index=['epsilon'])\n",
    "for iterator in figure6.columns:\n",
    "\n",
    "    # Ugh, don't need to specify float in Python 3+:\n",
    "    epsilon = float(iterator) / float(100)\n",
    "\n",
    "    # Define columns {A,B,C,D,E}:\n",
    "    x_A = pd.DataFrame(index=x_all.index, data=x_all.iloc[:, 2].values, columns=['x_A'])\n",
    "    x_B = pd.DataFrame(index=x_all.index, data=( \\\n",
    "                (1 - epsilon) * x_all.iloc[:, 2].values + \\\n",
    "                (epsilon * x_all.iloc[:, 4].values) \\\n",
    "        ), columns=['x_B'])\n",
    "    x_D = pd.DataFrame(index=x_all.index, data=( \\\n",
    "                (1 - epsilon) * x_all.iloc[:, 2].values + \\\n",
    "                (epsilon * x_all.iloc[:, 0].values) \\\n",
    "        ), columns=['x_D'])\n",
    "    x_C = pd.DataFrame(index=x_all.index, data=x_all.iloc[:, 3].values, columns=['x_C'])\n",
    "    x_E = pd.DataFrame(index=x_all.index, data=( \\\n",
    "                (1 - epsilon) * x_all.iloc[:, 3].values + \\\n",
    "                (epsilon * x_all.iloc[:, 1].values) \\\n",
    "        ), columns=['x_E'])\n",
    "\n",
    "    # Normalize all columns to have variance of 1:\n",
    "    x_Az = x_A / x_A.std()\n",
    "    x_Bz = x_B / x_B.std()\n",
    "    x_Cz = x_C / x_C.std()\n",
    "    x_Dz = x_D / x_D.std()\n",
    "    x_Ez = x_E / x_E.std()\n",
    "\n",
    "    x_concat = [x_Az, x_Bz, x_Cz, x_Dz, x_Ez]\n",
    "    x = pd.concat(x_concat, axis=1)\n",
    "\n",
    "    # Get covariance to be input to HRP:\n",
    "    covariances, corr = x.cov(), x.corr()\n",
    "\n",
    "    # Get HRP Weights:\n",
    "    dist = hrpm.correlDist(corr)\n",
    "    link = sch.linkage(dist, 'single')\n",
    "    sortIx = hrpm.getQuasiDiag(link)\n",
    "    sortIx = corr.index[sortIx].tolist()  # recover labels\n",
    "    df0 = corr.loc[sortIx, sortIx]  # reorder\n",
    "\n",
    "    # Capital allocation of HRP weights:\n",
    "    # B is in the lead here - why does it get highest weight...\n",
    "    hrp_s = hrpm.getRecBipart(covariances, sortIx)\n",
    "    hrp_df = pd.DataFrame(index=hrp_s.index, data=hrp_s.values, columns=['wts'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?\n",
    "\n",
    "Well, at the first bisection, we have the 5 assets separated as {C, E} and {B, A, D}.  So far, so good.  C+E are correlated to each other, and {A, B, D} are correlated.  And since all 5 assets have variance 1.00, an IVP allocation will assign 50% capital to the {C, E} superset and 50% capital to the {B, A, D} superset.\n",
    "\n",
    "However, at the next step of the recursion, we separate {B, A, D} into {B} + {A, D}.  Again, each asset has variance 1.00, and an IVP allocation will assign 50% capital to the {B} set and 50% capital to the {A, D} set.\n",
    "\n",
    "That's why {B} gets twice the weight of {A} and {D}, even though the three {A, B, D} are essentially the same asset, just slightly perturbed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix E: Solving Mystery 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the data set corresponding to epsilon = 0.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['x_D', 'x_A', 'x_B', 'x_C', 'x_E']]\n",
      "-----\n",
      "cItems0\n",
      "['x_D', 'x_A']\n",
      "-----\n",
      "cItems1\n",
      "['x_B', 'x_C', 'x_E']\n",
      "-----\n",
      "cVar0\n",
      "1.0000000000000013\n",
      "-----\n",
      "cVar1\n",
      "0.55527382075709\n",
      "-----\n",
      "alpha\n",
      "0.3570264048338372\n",
      "-----\n",
      "cItems0\n",
      "['x_D']\n",
      "-----\n",
      "cItems1\n",
      "['x_A']\n",
      "-----\n",
      "cVar0\n",
      "1.0000000000000013\n",
      "-----\n",
      "cVar1\n",
      "1.0000000000000013\n",
      "-----\n",
      "alpha\n",
      "0.5\n",
      "-----\n",
      "cItems0\n",
      "['x_B']\n",
      "-----\n",
      "cItems1\n",
      "['x_C', 'x_E']\n",
      "-----\n",
      "cVar0\n",
      "1.0000000000000013\n",
      "-----\n",
      "cVar1\n",
      "0.9999999999999964\n",
      "-----\n",
      "alpha\n",
      "0.4999999999999988\n",
      "-----\n",
      "cItems0\n",
      "['x_C']\n",
      "-----\n",
      "cItems1\n",
      "['x_E']\n",
      "-----\n",
      "cVar0\n",
      "0.9999999999999964\n",
      "-----\n",
      "cVar1\n",
      "0.9999999999999966\n",
      "-----\n",
      "alpha\n",
      "0.5\n",
      "-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kyle\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n"
     ]
    }
   ],
   "source": [
    "# Iterate over several values of epsilon and collect all outputs into single dataframe:\n",
    "iterator_array = pd.DataFrame(range(0,20,5))\n",
    "# Initialize:\n",
    "figure6 = pd.DataFrame(columns=list(range(0,1,10)), data=None, \\\n",
    "                               index=['epsilon'])\n",
    "for iterator in figure6.columns:\n",
    "\n",
    "    # Ugh, don't need to specify float in Python 3+:\n",
    "    epsilon = float(iterator) / float(100)\n",
    "\n",
    "    # Define columns {A,B,C,D,E}:\n",
    "    x_A = pd.DataFrame(index=x_all.index, data=x_all.iloc[:, 2].values, columns=['x_A'])\n",
    "    x_B = pd.DataFrame(index=x_all.index, data=( \\\n",
    "                (1 - epsilon) * x_all.iloc[:, 2].values + \\\n",
    "                (epsilon * x_all.iloc[:, 4].values) \\\n",
    "        ), columns=['x_B'])\n",
    "    x_D = pd.DataFrame(index=x_all.index, data=( \\\n",
    "                (1 - epsilon) * x_all.iloc[:, 2].values + \\\n",
    "                (epsilon * x_all.iloc[:, 0].values) \\\n",
    "        ), columns=['x_D'])\n",
    "    x_C = pd.DataFrame(index=x_all.index, data=x_all.iloc[:, 3].values, columns=['x_C'])\n",
    "    x_E = pd.DataFrame(index=x_all.index, data=( \\\n",
    "                (1 - epsilon) * x_all.iloc[:, 3].values + \\\n",
    "                (epsilon * x_all.iloc[:, 1].values) \\\n",
    "        ), columns=['x_E'])\n",
    "\n",
    "    # Normalize all columns to have variance of 1:\n",
    "    x_Az = x_A / x_A.std()\n",
    "    x_Bz = x_B / x_B.std()\n",
    "    x_Cz = x_C / x_C.std()\n",
    "    x_Dz = x_D / x_D.std()\n",
    "    x_Ez = x_E / x_E.std()\n",
    "\n",
    "    x_concat = [x_Az, x_Bz, x_Cz, x_Dz, x_Ez]\n",
    "    x = pd.concat(x_concat, axis=1)\n",
    "\n",
    "    # Get covariance to be input to HRP:\n",
    "    covariances, corr = x.cov(), x.corr()\n",
    "\n",
    "    # Get HRP Weights:\n",
    "    dist = hrpm.correlDist(corr)\n",
    "    link = sch.linkage(dist, 'single')\n",
    "    sortIx = hrpm.getQuasiDiag(link)\n",
    "    sortIx = corr.index[sortIx].tolist()  # recover labels\n",
    "    df0 = corr.loc[sortIx, sortIx]  # reorder\n",
    "\n",
    "    # Capital allocation of HRP weights:\n",
    "    # B is in the lead here - why does it get highest weight...\n",
    "    hrp_s = hrpm.getRecBipart(covariances, sortIx)\n",
    "    hrp_df = pd.DataFrame(index=hrp_s.index, data=hrp_s.values, columns=['wts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?\n",
    "\n",
    "Well, at the first bisection, we have the 5 assets separated as {A, D} and {B, C, E}.  \n",
    "\n",
    "This is different than Mystery 1.  So we're already off to a different start.  {A, B, D} are correlated to each other, but that cluster is destroyed at the first bisection.  \n",
    "\n",
    "Unlike Mystery 1, the sets {A, D} and {B, C, E} do not receive equal weights.  {A, D} gets \"alpha\" (weight) equal to 0.357, and {B, C, E} gets 64.3% capital (1 minus \"alpha\") to start the race to the bottom of the tree.\n",
    "\n",
    "When we bisect {B, C, E}, we create the subsets {B} & {C, E}.  Each gets equal allocation (half of 64.3%).\n",
    "\n",
    "That's why B ends up with a whopping 32% weight for epsilon = 0, but a drastically different weight of 25% for epsilon = 0.1; the smallest perturbation can affect outputs.  \n",
    "\n",
    "And since HRP doesn't really account for correlations, despite JoPM's claims, we get ridiculous results like this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
